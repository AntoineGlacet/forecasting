{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction of planned CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction step for wrong year in CSVs\n",
    "#  all_files = list(\n",
    "#     Path(\"/home/antoine/projects/forecasting/data/planned flights\").glob(\"*.csv\")\n",
    "# )\n",
    "# col_names = ['[Arr] ELDT', '[Arr] ALDT', '[Arr] SIBT',\n",
    "#        '[Arr] AIBT','[Dep] SOBT', '[Dep] AOBT', '[Dep] EDCT',\n",
    "#        '[Dep] ATOT', ]\n",
    "# for i,path in enumerate(all_files):\n",
    "#     csv = pd.read_csv(path)\n",
    "#     for col_name in col_names:\n",
    "#         csv.loc[csv[col_name].notna(),col_name] = csv.loc[csv[col_name].notna(),\n",
    "#             col_name\n",
    "#         ].apply(lambda x: x[0:3] + \"3\" + x[4:])\n",
    "\n",
    "#     csv.to_csv(Path('/home/antoine/projects/forecasting/data/planned_flight_corrected')/path.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_airports = pd.read_csv(\n",
    "    \"/home/antoine/projects/forecasting/data/AODB_airport_master.csv\"\n",
    ")\n",
    "\n",
    "data_countries = pd.read_csv(\n",
    "    \"/home/antoine/projects/forecasting/data/AODB_country_master.csv\"\n",
    ")\n",
    "\n",
    "all_files = list(\n",
    "    Path(\"/home/antoine/projects/forecasting/data/planned flights\").glob(\"*.csv\")\n",
    ")\n",
    "data_planned = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "\n",
    "all_files = list(\n",
    "    Path(\"/home/antoine/projects/forecasting/data/KIX_AODB_data\").glob(\"*.csv\")\n",
    ")\n",
    "data_actual = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning & augmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data_actual, training: bool = True):\n",
    "    # generate proper format\n",
    "    mask = data_actual[\"[Dep] Flight Designator\"].notna()\n",
    "    data_dep = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"L Board Pax\": pd.to_numeric(\n",
    "                data_actual[mask][\"[Dep]  L Board Pax\"], errors=\"coerce\"\n",
    "            ).to_list(),\n",
    "            \"Routing-FirstLeg\": data_actual[mask][\"[Dep] Routing\"]\n",
    "            .apply(lambda x: x[-4:])\n",
    "            .to_list(),\n",
    "            \"Datetime\": pd.to_datetime(data_actual[mask][\"[Dep] SOBT\"]).to_list(),\n",
    "            \"Service Type\": data_actual[mask][\"[Dep] Service Type\"].to_list(),\n",
    "            \"Traffic Type\": data_actual[mask][\"[Dep] Traffic Type\"].to_list(),\n",
    "            \"UniqueID\": pd.to_datetime(data_actual[mask][\"[Dep] SOBT\"]).dt.strftime(\n",
    "                \"%Y/%m/%d\"\n",
    "            )\n",
    "            + \" \"\n",
    "            + data_actual[mask][\"[Dep] Flight Designator\"],\n",
    "            # \"Capacity\": pd.to_numeric(\n",
    "            #     data_actual[mask][\"[Dep] Capacity\"], errors=\"coerce\"\n",
    "            # ).to_list(),\n",
    "        }\n",
    "    )\n",
    "    data_dep[\"Direction\"] = \"departure\"\n",
    "\n",
    "    mask = data_actual[\"[Arr] Flight Designator\"].notna()\n",
    "    data_arr = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"L Board Pax\": pd.to_numeric(\n",
    "                data_actual[mask][\"[Arr]  L Board Pax\"], errors=\"coerce\"\n",
    "            ).to_list(),\n",
    "            \"Routing-FirstLeg\": data_actual[mask][\"[Arr] Routing\"]\n",
    "            .apply(lambda x: x[0:4])\n",
    "            .to_list(),\n",
    "            \"Datetime\": pd.to_datetime(data_actual[mask][\"[Arr] SIBT\"]).to_list(),\n",
    "            \"Service Type\": data_actual[mask][\"[Arr] Service Type\"].to_list(),\n",
    "            \"Traffic Type\": data_actual[mask][\"[Arr] Traffic Type\"].to_list(),\n",
    "            \"UniqueID\": pd.to_datetime(data_actual[mask][\"[Arr] SIBT\"]).dt.strftime(\n",
    "                \"%Y/%m/%d\"\n",
    "            )\n",
    "            + \" \"\n",
    "            + data_actual[mask][\"[Arr] Flight Designator\"],\n",
    "            # \"Capacity\": pd.to_numeric(\n",
    "            #     data_actual[mask][\"[Arr] Capacity\"], errors=\"coerce\"\n",
    "            # ).to_list(),\n",
    "        }\n",
    "    )\n",
    "    data_arr[\"Direction\"] = \"arrival\"\n",
    "\n",
    "    data_concat = pd.concat([data_dep, data_arr]).reset_index(drop=True)\n",
    "    data_concat.drop_duplicates(subset=[\"UniqueID\"], inplace=True)\n",
    "    data = data_concat.copy()\n",
    "\n",
    "    # select only useful columns\n",
    "    data = data[\n",
    "        [\n",
    "            \"Service Type\",  # string\n",
    "            \"Traffic Type\",  # string\n",
    "            # \"Capacity\",  # to convert to int\n",
    "            \"L Board Pax\",  # int already\n",
    "            \"Direction\",  # string\n",
    "            \"Datetime\",  # date to convert to int for year/month/date/hour\n",
    "            \"Routing-FirstLeg\",  # string, should be country\n",
    "            \"UniqueID\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    # filter out 26 December\n",
    "    mask = data[\"Datetime\"].dt.floor(\"d\") != pd.to_datetime(\"2022-12-26\").floor(\"d\")\n",
    "    data = data[mask].copy()\n",
    "    # filter out rows with irrelevant values\n",
    "    mask = data[\"Service Type\"].isin([\"C\", \"G\", \"J\"])\n",
    "    data = data[mask].copy()\n",
    "\n",
    "    # change capacity to numerical\n",
    "    # data[\"Capacity\"] = pd.to_numeric(data[\"Capacity\"], errors=\"coerce\")\n",
    "    data[\"L Board Pax\"] = pd.to_numeric(data[\"L Board Pax\"], errors=\"coerce\")\n",
    "\n",
    "    # split date into year month day\n",
    "    data[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"])\n",
    "    data[\"Year\"] = data[\"Datetime\"].apply(lambda x: x.year)\n",
    "    data[\"Month\"] = data[\"Datetime\"].apply(lambda x: x.month)\n",
    "    data[\"Day\"] = data[\"Datetime\"].apply(lambda x: x.day)\n",
    "    data[\"Hour\"] = data[\"Datetime\"].apply(lambda x: x.hour)\n",
    "    data[\"LinearDate\"] = (\n",
    "        pd.to_datetime(data[\"Datetime\"]) - pd.to_datetime(\"2022-06-01\")\n",
    "    ) / np.timedelta64(1, \"D\")\n",
    "\n",
    "    # for training, take relevant flights and calculate LF\n",
    "    if training:\n",
    "        mask = data[\"L Board Pax\"] > 10\n",
    "        data = data[mask].copy()\n",
    "        # data.dropna(inplace=True)\n",
    "        data[\"L Board Pax\"] = data[\"L Board Pax\"].astype(\"int\")\n",
    "        # data[\"Load Factor\"] = data[\"L Board Pax\"] / data[\"Capacity\"]\n",
    "        # data = data[(data[\"Load Factor\"]<1) & (data[\"Load Factor\"]>0.1) ]\n",
    "    else:\n",
    "        data[\"L Board Pax\"] = np.nan\n",
    "    #     # mask = data[\"Capacity\"] > 80\n",
    "    #     # data = data[mask].copy()\n",
    "    #     # data[\"Load Factor\"] = np.nan\n",
    "\n",
    "    # change routing to Country name then to Country code\n",
    "    repl = (\n",
    "        data_airports[[\"ICAO\", \"Country\"]].set_index(\"ICAO\").T.to_dict(orient=\"records\")\n",
    "    )\n",
    "    data[\"Country\"] = data[\"Routing-FirstLeg\"].map(*repl)\n",
    "    repl_country = (\n",
    "        data_countries[[\"Name\", \"ISO-3166-1 alpha-2\"]]\n",
    "        .set_index(\"Name\")\n",
    "        .T.to_dict(orient=\"records\")\n",
    "    )\n",
    "    data[\"Country\"] = data[\"Country\"].map(*repl_country)\n",
    "\n",
    "    # holidays\n",
    "    data[\"HolidayJP\"] = 0\n",
    "    data[\"HolidayOrigin\"] = 0\n",
    "\n",
    "    dct_holiday = {\n",
    "        country_code: holidays.country_holidays(country_code)\n",
    "        for country_code in data[\"Country\"].unique()\n",
    "        if hasattr(holidays, country_code)\n",
    "    }\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        # domestic holiday\n",
    "        if row[\"Datetime\"] in dct_holiday[\"JP\"]:\n",
    "            data.loc[index, \"HolidayJP\"] = 1\n",
    "        # overseas holiday\n",
    "        if row[\"Country\"] in dct_holiday.keys():\n",
    "            if row[\"Datetime\"] in dct_holiday[row[\"Country\"]]:\n",
    "                data.loc[index, \"HolidayOrigin\"] = 1\n",
    "\n",
    "    # drop old columns\n",
    "    data.drop(\n",
    "        [\n",
    "            # \"L Board Pax\",\n",
    "            # \"Capacity\",\n",
    "            \"Routing-FirstLeg\",\n",
    "            # \"Datetime\",\n",
    "        ],\n",
    "        axis=\"columns\",\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # change types for categories\n",
    "    data[\"Service Type\"] = data[\"Service Type\"].astype(\"category\")\n",
    "    data[\"Traffic Type\"] = data[\"Traffic Type\"].astype(\"category\")\n",
    "    data[\"Direction\"] = data[\"Direction\"].astype(\"category\")\n",
    "    data[\"Country\"] = data[\"Country\"].astype(\"category\")\n",
    "\n",
    "    data[\"Month\"] = data[\"Month\"].astype(\"category\")\n",
    "    data[\"Day\"] = data[\"Day\"].astype(\"category\")\n",
    "    data[\"Hour\"] = data[\"Hour\"].astype(\"category\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline creation and first fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline creation\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "model = XGBRegressor()\n",
    "# model = LGBMRegressor()\n",
    "regressor = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "data_actual_clean = data_cleaning(data_actual)\n",
    "# split dataset\n",
    "X = data_actual_clean.drop([\"L Board Pax\", \"UniqueID\", \"Datetime\"], axis=1)\n",
    "y = data_actual_clean[\"L Board Pax\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "print(\"training score: %.3f\" % regressor.score(X_train, y_train))\n",
    "print(\"model score: %.3f\" % regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for lgbm\n",
    "# feat_importances = pd.Series(\n",
    "#     model.feature_importances_, index=regressor[\"preprocessor\"].get_feature_names_out()\n",
    "# )\n",
    "# feat_importances.nlargest(10).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor[\"regressor\"].get_booster().feature_names = list(\n",
    "    regressor[\"preprocessor\"].get_feature_names_out()\n",
    ")\n",
    "xgb.plot_importance(regressor[\"regressor\"], ax=plt.gca(), max_num_features=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "hyperparameter_grid = {\n",
    "    # 'regressor__n_estimators': [100, 400, 800],\n",
    "    # 'regressor__max_depth': [3, 6, 9],\n",
    "    \"regressor__learning_rate\": [0.30, 0.35, 0.5, 0.6],\n",
    "    \"regressor__min_child_weight\": [1, 5, 10],\n",
    "}\n",
    "\n",
    "# gridCV = GridSearchCV(regressor, param_grid=hyperparameter_grid, cv=4)\n",
    "# # gridCV.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters set found on development set:\")\n",
    "# print(gridCV.best_params_)\n",
    "# print(\"Best score found on development set:\")\n",
    "# print(gridCV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(learning_rate=0.5)\n",
    "regressor = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", model)])\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "print(\"training score: %.3f\" % regressor.score(X_train, y_train))\n",
    "print(\"model score: %.3f\" % regressor.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on future schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on January\n",
    "data_forecast_clean = data_cleaning(data_planned, training=False)\n",
    "X_forecast = data_forecast_clean.drop([\"L Board Pax\", \"UniqueID\", \"Datetime\"], axis=1)\n",
    "preds = regressor.predict(X_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forecast_clean[\"L Board Pax\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_actual_result = data_actual_clean.set_index(\"Datetime\", drop=True)\n",
    "mask = (data_actual_result[\"Traffic Type\"] == \"INTERNATIONAL\") & (\n",
    "    data_actual_result[\"Direction\"] == \"departure\"\n",
    ")\n",
    "plot_actual = data_actual_result[mask][\"L Board Pax\"].groupby(\n",
    "    pd.to_datetime(data_actual_result[mask].index).date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forecast_result = data_forecast_clean.set_index(\"Datetime\", drop=True)\n",
    "mask = (data_forecast_result[\"Traffic Type\"] == \"INTERNATIONAL\") & (\n",
    "    data_forecast_result[\"Direction\"] == \"departure\"\n",
    ")\n",
    "plot_forecast = data_forecast_result[mask][\"L Board Pax\"].groupby(\n",
    "    pd.to_datetime(data_forecast_result[mask].index).date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].plot(plot_forecast.agg(\"count\"))\n",
    "ax[0].plot(plot_actual.agg(\"count\"))\n",
    "ax[1].plot(plot_forecast.agg(\"sum\"))\n",
    "ax[1].plot(plot_actual.agg(\"sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"/home/antoine/projects/forecasting/output\") / \"forecast.csv\"\n",
    "data_forecast_result[[\"UniqueID\", \"L Board Pax\"]].reset_index().to_csv(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5cde2381dd949eede0613f4452aac5627aca93dd18e02f051004af15b34586f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
